{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping the API into a production container\n",
    "\n",
    "In the previous chapter, we have created an API that can log requests and return responses locally. We need to wrap everything up into a docker container to make it available to the outside world.\n",
    "\n",
    "First off all, lets see the file structure and recap what is the logic behind each of the file and directory in the project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mML_API_docker\u001b[00m\r\n",
      "├── \u001b[01;34malembic\u001b[00m\r\n",
      "│   ├── env.py\r\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "│   ├── README\r\n",
      "│   ├── script.py.mako\r\n",
      "│   └── \u001b[01;34mversions\u001b[00m\r\n",
      "├── alembic.ini\r\n",
      "├── app.py\r\n",
      "├── \u001b[01;34mdatabase\u001b[00m\r\n",
      "│   ├── config.yml\r\n",
      "│   ├── database.py\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── jwt_tokens.py\r\n",
      "│   ├── ML.py\r\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "│   └── Users.py\r\n",
      "├── \u001b[01;34mdata_docker\u001b[00m\r\n",
      "│   └── \u001b[01;34mdb\u001b[00m\r\n",
      "├── docker-compose.yml\r\n",
      "├── Dockerfile\r\n",
      "├── gunicorn_config.conf\r\n",
      "├── __init__.py\r\n",
      "├── \u001b[01;34mml_model\u001b[00m\r\n",
      "│   ├── input_schema.json\r\n",
      "│   └── model.pkl\r\n",
      "└── requirements.txt\r\n",
      "\r\n",
      "8 directories, 18 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ML_API_docker -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `alembic` directory manages the migrations and that the changes in the python files get reflected in the database.\n",
    "\n",
    "The `alembic.ini` file is the configuration file for the `alembic` package. It holds the connection string to the database and the location of the migration scripts. In a real life application you should not track this file because it contains the full connection URI. \n",
    "\n",
    "The `app.py` file is the main file of the API. It imports all the necesary logic to run and creates the FlaskAPI object which is used in runtime. \n",
    "\n",
    "The `database` directory holds all the database models and other associated logic. \n",
    "\n",
    "The `data_docker` directory links the data from PSQL in a docker container to the local machine. This way, if restart the container, all the data is saved and we do not need to rerun all the migrations.\n",
    "\n",
    "The `docker-compose.yml` file creates the two containers: 1 for the PSQL service and 1 for the API service.\n",
    "\n",
    "The `Dockerfile` handles the image creation for the API application. \n",
    "\n",
    "The `gunicorn_config.conf` is a configuration file for the supervisor application to serve the API using gunicorn.\n",
    "\n",
    "The `__init__.py` file indicates for python that the whole directory is a package. This makes certain imports not break. \n",
    "\n",
    "The `ml_model` holds the files necesary for the machine learning model.\n",
    "\n",
    "Lastly, the `requirements.txt` file holds the dependencies for the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerfile\n",
    "\n",
    "The dockerfile commands will create an image which can be used to spin up the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Base image. We will use the Ubuntu base image and populate it \r\n",
      "FROM ubuntu:20.04 \r\n",
      "\r\n",
      "# Updating all the base packages and installing python and pip \r\n",
      "RUN apt-get update && apt-get install -y python3-pip\r\n",
      "\r\n",
      "# Installing supervisor to manage the API processes\r\n",
      "RUN apt-get install -y supervisor\r\n",
      "\r\n",
      "# Creating the working directory **inside** the container. All the subsequent commands will be executed in this directory. \r\n",
      "WORKDIR /app\r\n",
      "\r\n",
      "# Copying the requirements.txt file to the container. The . means that copy everything to the current WORKDIR (which is /app) \r\n",
      "COPY requirements.txt .\r\n",
      "\r\n",
      "# Installing every package in the requirements.txt file\r\n",
      "RUN pip3 install -r requirements.txt\r\n",
      "\r\n",
      "# Copying over the app code to the container\r\n",
      "COPY app.py . \r\n",
      "\r\n",
      "# Copying the database functionalities\r\n",
      "COPY database/ /app/database/ \r\n",
      "\r\n",
      "# Copying the ml_model folder to the container \r\n",
      "COPY ml_model/ /app/ml_model/\r\n",
      "\r\n",
      "# Copying the configuration for the supervisor process \r\n",
      "COPY gunicorn_config.conf /etc/supervisor/conf.d/gunicorn_config.conf\r\n",
      "\r\n",
      "# Running the gunicorn process\r\n",
      "CMD [\"/usr/bin/supervisord\", \"-c\", \"/etc/supervisor/supervisord.conf\"]"
     ]
    }
   ],
   "source": [
    "!cat ML_API_docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the image use the command: \n",
    "\n",
    "```\n",
    "docker build -t ml-api .\n",
    "```\n",
    "\n",
    "# Running the containers\n",
    "\n",
    "The built image is used in the docker-compose.yml file alongside another container for psql: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: '3.1'\r\n",
      "\r\n",
      "services:\r\n",
      "\r\n",
      "  psql_db:\r\n",
      "    image: postgres:14.1\r\n",
      "    restart: always\r\n",
      "    environment:\r\n",
      "      POSTGRES_PASSWORD: password\r\n",
      "      POSTGRES_USER: user \r\n",
      "    ports:\r\n",
      "      - \"5444:5432\"\r\n",
      "    volumes:\r\n",
      "      - ./data_docker/db:/var/lib/postgresql/data\r\n",
      "\r\n",
      "  ml_api:\r\n",
      "    image: ml-api\r\n",
      "    restart: always\r\n",
      "    ports:\r\n",
      "      - \"8999:8900\"\r\n",
      "    "
     ]
    }
   ],
   "source": [
    "!cat ML_API_docker/docker-compose.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The container for the ml-api will will link requests from 8999 port on the local machine to port 8900 inside the container. \n",
    "\n",
    "If the container goes down for some reason, the docker background process will restart it.\n",
    "\n",
    "To spin up both the containers use the command: \n",
    "\n",
    "```\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "Be sure to make the necesary migrations if this is the initial run of the containers: \n",
    "\n",
    "```\n",
    "alembic revision -m \"Creating migrations\" --autogenerate\n",
    "```\n",
    "\n",
    "And apply them: \n",
    "\n",
    "```\n",
    "alembic upgrade head\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level schema \n",
    "\n",
    "To get a better view of what processes get start after the command `docker-compose up`, lets illustrate the high level schema:\n",
    "\n",
    "![api-flow](media/api-production.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the containers are managed by docker. \n",
    "\n",
    "The only port where the requests can come in for the machine learning API is through the port **8999**. All the requests get redirected to the port **8900** inside the container. From there, gunicorn gives the request to one of the workers. \n",
    "\n",
    "The container with the PSQL database can be reached via port **5444** on the local machine. The data gets redirected to the port **5432** inside the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the API from the container\n",
    "\n",
    "All the API calls will be done to the container which is running on the local machine.\n",
    "\n",
    "## Creating a user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 409; Response: {'message': 'User already exists', 'user_id': 1}\n"
     ]
    }
   ],
   "source": [
    "# Importing the package \n",
    "import requests\n",
    "\n",
    "# Defining the URL \n",
    "url = 'http://localhost:8999'\n",
    "\n",
    "# Defining the user dict \n",
    "user_dict = {\n",
    "    \"username\": \"eligijus_bujokas\",\n",
    "    \"password\": \"password\",\n",
    "    \"email\": \"eligijus@testmail.com\"\n",
    "}\n",
    "\n",
    "# Sending the post request to the running API \n",
    "response = requests.post(f\"{url}/register-user\", json=user_dict)\n",
    "\n",
    "# Getting the user id \n",
    "user_id = response.json().get(\"user_id\")\n",
    "\n",
    "# Printing the response \n",
    "print(f\"Response code: {response.status_code}; Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200; JWT token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE2NDIzNDA5NjgsImlhdCI6MTY0MjMzNzM2OCwic3ViIjoxfQ.2A8H9Z0XA9H-dX6wP31aQRCaRaVdNHgsFD1zr5hafyo\n"
     ]
    }
   ],
   "source": [
    "# Registering the user to docker \n",
    "response = requests.post(f\"{url}/token\", json={\"username\": \"eligijus_bujokas\", \"password\": \"password\"})\n",
    "\n",
    "# Extracting the token from the response\n",
    "token = response.json().get(\"token\")\n",
    "\n",
    "# Printing the response\n",
    "print(f\"Response code: {response.status_code}; JWT token: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200; Response: {'yhat_prob': '0.5124506', 'yhat': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Creating the input dictionary\n",
    "X = {\n",
    "    'age': 25,\n",
    "    'creatinine_phosphokinase': 1000,\n",
    "    'ejection_fraction': 35,\n",
    "    'platelets': 500000,\n",
    "    'serum_creatinine': 8,\n",
    "    'serum_sodium': 135,\n",
    "    'sex': 1,\n",
    "    'high_blood_pressure': 0\n",
    "}\n",
    "\n",
    "# Creating the header with the token \n",
    "header = {\n",
    "    'Authorization': token\n",
    "}\n",
    "\n",
    "# Sending the request \n",
    "response = requests.post(f\"{url}/predict\", json=X, headers=header)\n",
    "\n",
    "# Infering the response\n",
    "print(f\"Response code: {response.status_code}; Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "The container accepts requests via the port 8999. If we have a running docker background process on any server, we can spin up this container and use the machine learning model imediatly. \n",
    "\n",
    "The API itself is served using `gunicorn` with **n** workers. \n",
    "\n",
    "Each worker is an `uvicorn` async server that will handle the requests. \n",
    "\n",
    "Gunicorn itself is managed using `supervisor`. \n",
    "\n",
    "If the container breaks, then docker daemon will automatically restart it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributions \n",
    "\n",
    "If you enjoyed the book and feel like donating, feel free to do so. The link to do a one time donation is [via Stripe](https://buy.stripe.com/14k17A6lQ8lAat2aEI). \n",
    "\n",
    "Additionaly, if you want me to add another chapter or to expand an existing one, please create an issue on [Github](https://github.com/Eligijus112/api-book)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "702d029752c7c667e866081f4be82ec9765259a2e8484bced05e549319c2e426"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('api_book': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}