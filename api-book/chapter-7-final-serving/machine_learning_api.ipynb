{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning API\n",
    "\n",
    "Right now we have 3 database tables: \n",
    "\n",
    "* Users\n",
    "* Requests\n",
    "* Responses\n",
    "\n",
    "Additionaly, we have created a machine learning model along with the input schema. It is time to create a working API using FastAPI to serve predictions. \n",
    "\n",
    "# Loading the ML model to memory \n",
    "\n",
    "The most efficient way to load an ML model to memory is to save it during the initiation of the FastAPI application. It is a common mistake to read the model file and the schema file everytime a new request comes in and then apply it. \n",
    "\n",
    "We should import the model objects and create any additional objects at the top of the main **app.py** script where the API object is beeing created. \n",
    "\n",
    "The necessary utilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pickle object reading \n",
      "import pickle \n",
      "\n",
      "# JSON object reading \n",
      "import json \n",
      "\n",
      "# OS traversal \n",
      "import os \n",
      "\n",
      "# Input dataframe\n",
      "import pandas as pd \n",
      "\n",
      "# Array math \n",
      "import numpy as np \n",
      "\n",
      "def load_ml_model(model_dir='ml_model'):\n",
      "    \"\"\"\n",
      "    Loads the model and the schema from the given path\n",
      "    \"\"\"\n",
      "    model, type_dict, feature_list = {}, {}, []\n",
      "    \n",
      "    _model_path = os.path.join(model_dir, 'model.pkl')\n",
      "    _input_schema_path = os.path.join(model_dir, 'input_schema.json')\n",
      "\n",
      "    # Checking if the files exists and reading them \n",
      "    if os.path.exists(_model_path) and os.path.exists(_input_schema_path):\n",
      "        with open(_model_path, 'rb') as f:\n",
      "            model = pickle.load(f)\n",
      "        with open(_input_schema_path, 'r') as f:\n",
      "            input_schema = json.load(f)\n",
      "    \n",
      "    # Extracting the features\n",
      "    features = input_schema.get('input_schema', {})\n",
      "    features = features.get('columns', [])\n",
      "\n",
      "    # Iterating over the list of dictionaries and changing the types.\n",
      "    # numeric -> float \n",
      "    # boolean -> bool\n",
      "    # The resulting dictionary will have a key value of the feature name and the value will be the type\n",
      "    for feature in features:\n",
      "        if feature.get('type') == 'numeric':\n",
      "            feature['type'] = float\n",
      "        elif feature.get('type') == 'boolean':\n",
      "            feature['type'] = bool\n",
      "        type_dict.update({feature.get('name'): feature.get('type')})\n",
      "    \n",
      "    # Extracting the correct ordering of the features for the ML input \n",
      "    feature_list = [x.get('name') for x in features]\n",
      "\n",
      "    # Returning the model, type dictionary and the feature order\n",
      "    return model, type_dict, feature_list\n",
      "\n",
      "def predict(model, feature_dict: dict, X: pd.DataFrame) -> list:\n",
      "    \"\"\"\n",
      "    Function that converts the feature_dict into a predictable format for the ml model\n",
      "\n",
      "    Args:\n",
      "        model: the machine learning model\n",
      "        feature_dict: the dictionary of features\n",
      "        X: the features used in prediction\n",
      "\n",
      "    Returns:\n",
      "        A list of predictions\n",
      "    \"\"\"\n",
      "    # Converting the dictionary into a list of lists\n",
      "    feature_list = list(feature_dict.values())\n",
      "\n",
      "    # Ensuring that no columns are missing \n",
      "    if len(feature_list) != X.shape[1]:\n",
      "        for col in X.columns:\n",
      "            if col not in feature_list:\n",
      "                X[col] = np.nan\n",
      "\n",
      "    # Converting the X columns to correct types \n",
      "    for col in X.columns:\n",
      "        if col in feature_list:\n",
      "            try:\n",
      "                X[col] = X[col].astype(feature_dict.get(col))\n",
      "            except: \n",
      "                print(f\"Cannot convert {col} to {feature_dict.get(col)}\")\n",
      "                # If we cannot convert it, we will set it to null. \n",
      "                X[col] = np.nan\n",
      "\n",
      "    # Predicting the output\n",
      "    prediction = model.predict(X[feature_list])\n",
      "\n",
      "    return prediction"
     ]
    }
   ],
   "source": [
    "!cat ML_API/machine_learning_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading of the model occurs right before defining the endpoints:\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "# Creating the application object \n",
    "app = FastAPI()\n",
    "\n",
    "# Loading the machine learning objects to memory \n",
    "ml_model, type_dict, ml_feature_list = load_ml_model()\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "By loading the objects in the following way, the objects are saved in runtime memory and are not loaded from disk everytime a new request comes in. This makes the application much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API usage flowchart\n",
    "\n",
    "A typical flow of the API is the following: \n",
    "\n",
    "* Register a user: \n",
    "\n",
    "![registration](media/registration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the registration logic is a JWT token which we attach in each of the requests to our API. \n",
    "\n",
    "* Prediction flow: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![api-flow](media/API-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each request to the API needs to have the JWT token attached to it. Then, along with the token, the data for the API is sent ant the following flow starts: \n",
    "\n",
    "1) The user is beeing authenticated. \n",
    "\n",
    "2) If the user is authenticated, then the request data is beeing validated for the ML model. \n",
    "\n",
    "3) If the data is good, then the prediction is beeing made.\n",
    "\n",
    "4) The final response is sent. \n",
    "\n",
    "Along the way, the information is logged to the **Requests** and **Responses** tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code is available in the **app.py** script in the ML_API directory so lets try and apply the above flowchart!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API usage\n",
    "\n",
    "## Creating a user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "702d029752c7c667e866081f4be82ec9765259a2e8484bced05e549319c2e426"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('api_book': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
