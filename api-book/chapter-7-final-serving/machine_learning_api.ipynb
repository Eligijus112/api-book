{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning API\n",
    "\n",
    "Right now we have 3 database tables: \n",
    "\n",
    "* Users\n",
    "* Requests\n",
    "* Responses\n",
    "\n",
    "Additionaly, we have created a machine learning model along with the input schema. It is time to create a working API using FastAPI to serve predictions. \n",
    "\n",
    "# Loading the ML model to memory \n",
    "\n",
    "The most efficient way to load an ML model to memory is to save it during the initiation of the FastAPI application. It is a common mistake to read the model file and the schema file everytime a new request comes in and then apply it. \n",
    "\n",
    "We should import the model objects and create any additional objects at the top of the main **app.py** script where the API object is beeing created. \n",
    "\n",
    "The necessary utilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pickle object reading \n",
      "import pickle \n",
      "\n",
      "# JSON object reading \n",
      "import json \n",
      "\n",
      "# OS traversal \n",
      "import os \n",
      "\n",
      "# Input dataframe\n",
      "import pandas as pd \n",
      "\n",
      "# Array math \n",
      "import numpy as np \n",
      "\n",
      "def load_ml_model(model_dir='ml_model'):\n",
      "    \"\"\"\n",
      "    Loads the model and the schema from the given path\n",
      "    \"\"\"\n",
      "    model, type_dict, feature_list = {}, {}, []\n",
      "    \n",
      "    _model_path = os.path.join(model_dir, 'model.pkl')\n",
      "    _input_schema_path = os.path.join(model_dir, 'input_schema.json')\n",
      "\n",
      "    # Default empty input schema \n",
      "    input_schema = {}\n",
      "\n",
      "    # Checking if the files exists and reading them \n",
      "    if os.path.exists(_model_path) and os.path.exists(_input_schema_path):\n",
      "        \n",
      "        with open(_model_path, 'rb') as f:\n",
      "            model = pickle.load(f)\n",
      "\n",
      "        with open(_input_schema_path, 'r') as f:\n",
      "            input_schema = json.load(f)\n",
      "    \n",
      "    # Extracting the features\n",
      "    features = input_schema.get('input_schema', {})\n",
      "    features = features.get('columns', [])\n",
      "\n",
      "    # Iterating over the list of dictionaries and changing the types.\n",
      "    # numeric -> float \n",
      "    # boolean -> bool\n",
      "    # The resulting dictionary will have a key value of the feature name and the value will be the type\n",
      "    for feature in features:\n",
      "        if feature.get('type') == 'numeric':\n",
      "            feature['type'] = float\n",
      "        elif feature.get('type') == 'boolean':\n",
      "            feature['type'] = bool\n",
      "        type_dict.update({feature.get('name'): feature.get('type')})\n",
      "    \n",
      "    # Extracting the correct ordering of the features for the ML input \n",
      "    feature_list = [x.get('name') for x in features]\n",
      "\n",
      "    # Returning the model, type dictionary and the feature order\n",
      "    return model, type_dict, feature_list\n",
      "\n",
      "def predict(model, feature_dict: dict, X: dict) -> list:\n",
      "    \"\"\"\n",
      "    Function that converts the feature_dict into a predictable format for the ml model\n",
      "\n",
      "    Args:\n",
      "        model: the machine learning model\n",
      "        feature_dict: the dictionary of features\n",
      "        X: dictionary with (feature -> feature value) pairs\n",
      "\n",
      "    Returns:\n",
      "        A list of predictions\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Converting the dictionary into a list of lists\n",
      "        feature_list = list(feature_dict.keys())\n",
      "\n",
      "        # Converting the dictionary to a dataframe \n",
      "        X = pd.DataFrame(X, index=[0])\n",
      "\n",
      "        # Ensuring that no columns are missing \n",
      "        if len(feature_list) != X.shape[1]:\n",
      "            for col in X.columns:\n",
      "                if col not in feature_list:\n",
      "                    X[col] = np.nan\n",
      "\n",
      "        # Converting the X columns to correct types \n",
      "        for col in X.columns:\n",
      "            if col in feature_list:\n",
      "                try:\n",
      "                    X[col] = X[col].astype(feature_dict.get(col))\n",
      "                except: \n",
      "                    print(f\"Cannot convert {col} to {feature_dict.get(col)}\")\n",
      "                    # If we cannot convert it, we will set it to null. \n",
      "                    return None \n",
      "\n",
      "        # Predicting the output\n",
      "        prediction = model.predict_proba(X[feature_list])[0]\n",
      "\n",
      "        return prediction\n",
      "    except:\n",
      "        return None"
     ]
    }
   ],
   "source": [
    "!cat ML_API/machine_learning_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading of the model occurs right before defining the endpoints:\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "# Creating the application object \n",
    "app = FastAPI()\n",
    "\n",
    "# Loading the machine learning objects to memory \n",
    "ml_model, type_dict, ml_feature_list = load_ml_model()\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "By loading the objects in the following way, the objects are saved in runtime memory and are not loaded from disk everytime a new request comes in. This makes the application much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API usage flowchart\n",
    "\n",
    "A typical flow of the API is the following: \n",
    "\n",
    "* Register a user: \n",
    "\n",
    "![registration](media/registration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the registration logic is a JWT token which we attach in each of the requests to our API. \n",
    "\n",
    "* Prediction flow: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![api-flow](media/API-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each request to the API needs to have the JWT token attached to it. Then, along with the token, the data for the API is sent ant the following flow starts: \n",
    "\n",
    "1) The user is beeing authenticated. \n",
    "\n",
    "2) If the user is authenticated, then the request data is beeing validated for the ML model. \n",
    "\n",
    "3) If the data is good, then the prediction is beeing made.\n",
    "\n",
    "4) The final response is sent. \n",
    "\n",
    "Along the way, the information is logged to the **Requests** and **Responses** tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code is available in the **app.py** script in the ML_API directory so lets try and apply the above flowchart!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests making  \n",
    "import requests \n",
    "\n",
    "# Defining the constants for the API\n",
    "url = 'http://localhost:8081'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 409; Response: {'message': 'User already exists', 'user_id': 5}\n"
     ]
    }
   ],
   "source": [
    "# Defining the user dict \n",
    "user_dict = {\n",
    "    \"username\": \"eligijus_bujokas\",\n",
    "    \"password\": \"password\",\n",
    "    \"email\": \"eligijus@testmail.com\"\n",
    "}\n",
    "\n",
    "# Sending the post request to the running API \n",
    "response = requests.post(f\"{url}/register-user\", json=user_dict)\n",
    "\n",
    "# Getting the user id \n",
    "user_id = response.json().get(\"user_id\")\n",
    "\n",
    "# Printing the response \n",
    "print(f\"Response code: {response.status_code}; Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200; JWT token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE2NDIwMjQ0NTMsImlhdCI6MTY0MjAyMDg1Mywic3ViIjo1fQ.69QxEMTrcwT2wcatn1a_Ng6h6LOW4c48KQYzZnepOyk\n"
     ]
    }
   ],
   "source": [
    "# Querying the API for the token \n",
    "response = requests.post(f\"{url}/token\", json=user_dict)\n",
    "\n",
    "# Extracting the token from the response\n",
    "token = response.json().get(\"token\")\n",
    "\n",
    "# Printing the response\n",
    "print(f\"Response code: {response.status_code}; JWT token: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the predictions\n",
    "\n",
    "We need to first recap what was the input used to train the model. The features were: \n",
    "\n",
    "```\n",
    "{\n",
    "  \"input_schema\": {\n",
    "    \"columns\": [\n",
    "      {\n",
    "        \"name\": \"age\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"creatinine_phosphokinase\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"ejection_fraction\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"platelets\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"serum_creatinine\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"serum_sodium\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"sex\",\n",
    "        \"type\": \"boolean\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"high_blood_pressure\",\n",
    "        \"type\": \"boolean\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a POST request to get the probabilities because we want to send the features and their values not as a collection of URL parameters but as a JSON object in the request body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200; Response: {'yhat_prob': '0.5124506', 'yhat': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Creating the input dictionary\n",
    "X = {\n",
    "    'age': 25,\n",
    "    'creatinine_phosphokinase': 1000,\n",
    "    'ejection_fraction': 35,\n",
    "    'platelets': 500000,\n",
    "    'serum_creatinine': 8,\n",
    "    'serum_sodium': 135,\n",
    "    'sex': 1,\n",
    "    'high_blood_pressure': 0\n",
    "}\n",
    "\n",
    "# Creating the header with the token \n",
    "header = {\n",
    "    'Authorization': token\n",
    "}\n",
    "\n",
    "# Sending the request \n",
    "response = requests.post(f\"{url}/predict\", json=X, headers=header)\n",
    "\n",
    "# Infering the response\n",
    "print(f\"Response code: {response.status_code}; Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response dictionary has two keys: \n",
    "\n",
    "`yhat_prob` - probability of a death event \n",
    "\n",
    "`yhat` - the predicted class; 1 - death_event, 0 - no_death_event\n",
    "\n",
    "The bellow function handles the request and the whole logic is presented here. \n",
    "\n",
    "The steps are: \n",
    "\n",
    "1) Extract the token \n",
    "\n",
    "2) Authenticate it\n",
    "\n",
    "3) Extract the inputs\n",
    "\n",
    "4) Log the request to database \n",
    "\n",
    "5) Make the prediction \n",
    "\n",
    "6) Log the response to database\n",
    "\n",
    "7) Return the response to the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@app.post('/predict')\n",
      "async def predict_ml(request: Request):\n",
      "    \"\"\"\n",
      "    Endpoint for the ML model prediction\n",
      "    \"\"\"\n",
      "    # Extracting the token from the header\n",
      "    token = request.headers.get('Authorization')\n",
      "    if token:\n",
      "        # Authenticating the token\n",
      "        user = authenticate_token_view(token)\n",
      "        if user:\n",
      "            # Extracting the features from the request\n",
      "            input_dict = await request.json()\n",
      "            \n",
      "            # Creating the request object in database\n",
      "            ml_request = MLRequests(user_id=user.id, input=json.dumps(input_dict))\n",
      "            session.add(ml_request)\n",
      "            session.commit()\n",
      "\n",
      "            # Getting the prediction \n",
      "            prediction = predict(ml_model, type_dict, input_dict)\n",
      "\n",
      "            if prediction is not None:\n",
      "                # Creating the response dictionary \n",
      "                response_dict = {\n",
      "                    'yhat_prob': str(prediction[1]),\n",
      "                    'yhat': str(np.sum(prediction[1] > 0.5))\n",
      "                }\n",
      "\n",
      "                # Creating the response object in database\n",
      "                ml_response = MLResponses(request_id=ml_request.id, output=json.dumps(response_dict))\n",
      "                session.add(ml_response)\n",
      "                session.commit()\n",
      "\n",
      "                # Returning the prediction\n",
      "                return Response(status_code=status.HTTP_200_OK, content=json.dumps(response_dict))\n",
      "            else:\n",
      "                # Returning the message with the bad request status code\n",
      "                return Response(status_code=status.HTTP_400_BAD_REQUEST, content=json.dumps({'message': 'Bad request'}))\n",
      "        else:\n",
      "            # Returning a 401 Unauthorized error\n",
      "            return Response(status_code=status.HTTP_401_UNAUTHORIZED, content=json.dumps({'error': 'Unauthorized'}))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the prediction function\n",
    "from ML_API.app import predict_ml\n",
    "import inspect\n",
    "\n",
    "# Printing the source code\n",
    "print(inspect.getsource(predict_ml))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "702d029752c7c667e866081f4be82ec9765259a2e8484bced05e549319c2e426"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('api_book': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
