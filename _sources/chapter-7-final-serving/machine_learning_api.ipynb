{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning API\n",
    "\n",
    "Right now we have 3 database tables: \n",
    "\n",
    "* Users\n",
    "* Requests\n",
    "* Responses\n",
    "\n",
    "Additionaly, we have created a machine learning model along with the input schema. It is time to create a working API using FastAPI to serve predictions. \n",
    "\n",
    "# Loading the ML model to memory \n",
    "\n",
    "The most efficient way to load an ML model to memory is to save it during the initiation of the FastAPI application. It is a common mistake to read the model file and the schema file everytime a new request comes in and then apply it. \n",
    "\n",
    "We should import the model objects and create any additional objects at the top of the main **app.py** script where the API object is beeing created. \n",
    "\n",
    "The necessary utilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pickle object reading \n",
      "import pickle \n",
      "\n",
      "# JSON object reading \n",
      "import json \n",
      "\n",
      "# OS traversal \n",
      "import os \n",
      "\n",
      "# Input dataframe\n",
      "import pandas as pd \n",
      "\n",
      "# Array math \n",
      "import numpy as np \n",
      "\n",
      "def load_ml_model(model_dir='ml_model'):\n",
      "    \"\"\"\n",
      "    Loads the model and the schema from the given path\n",
      "    \"\"\"\n",
      "    model, type_dict, feature_list = {}, {}, []\n",
      "    \n",
      "    _model_path = os.path.join(model_dir, 'model.pkl')\n",
      "    _input_schema_path = os.path.join(model_dir, 'input_schema.json')\n",
      "\n",
      "    # Default empty input schema \n",
      "    input_schema = {}\n",
      "\n",
      "    # Checking if the files exists and reading them \n",
      "    if os.path.exists(_model_path) and os.path.exists(_input_schema_path):\n",
      "        \n",
      "        with open(_model_path, 'rb') as f:\n",
      "            model = pickle.load(f)\n",
      "\n",
      "        with open(_input_schema_path, 'r') as f:\n",
      "            input_schema = json.load(f)\n",
      "    \n",
      "    # Extracting the features\n",
      "    features = input_schema.get('input_schema', {})\n",
      "    features = features.get('columns', [])\n",
      "\n",
      "    # Iterating over the list of dictionaries and changing the types.\n",
      "    # numeric -> float \n",
      "    # boolean -> bool\n",
      "    # The resulting dictionary will have a key value of the feature name and the value will be the type\n",
      "    for feature in features:\n",
      "        if feature.get('type') == 'numeric':\n",
      "            feature['type'] = float\n",
      "        elif feature.get('type') == 'boolean':\n",
      "            feature['type'] = bool\n",
      "        type_dict.update({feature.get('name'): feature.get('type')})\n",
      "    \n",
      "    # Extracting the correct ordering of the features for the ML input \n",
      "    feature_list = [x.get('name') for x in features]\n",
      "\n",
      "    # Returning the model, type dictionary and the feature order\n",
      "    return model, type_dict, feature_list\n",
      "\n",
      "def predict(model, feature_dict: dict, X: dict) -> list:\n",
      "    \"\"\"\n",
      "    Function that converts the feature_dict into a predictable format for the ml model\n",
      "\n",
      "    Args:\n",
      "        model: the machine learning model\n",
      "        feature_dict: the dictionary of features\n",
      "        X: dictionary with (feature -> feature value) pairs\n",
      "\n",
      "    Returns:\n",
      "        A list of predictions\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Converting the dictionary into a list of lists\n",
      "        feature_list = list(feature_dict.keys())\n",
      "\n",
      "        # Converting the dictionary to a dataframe \n",
      "        X = pd.DataFrame(X, index=[0])\n",
      "\n",
      "        # Ensuring that no columns are missing \n",
      "        if len(feature_list) != X.shape[1]:\n",
      "            for col in X.columns:\n",
      "                if col not in feature_list:\n",
      "                    X[col] = np.nan\n",
      "\n",
      "        # Converting the X columns to correct types \n",
      "        for col in X.columns:\n",
      "            if col in feature_list:\n",
      "                try:\n",
      "                    X[col] = X[col].astype(feature_dict.get(col))\n",
      "                except: \n",
      "                    print(f\"Cannot convert {col} to {feature_dict.get(col)}\")\n",
      "                    # If we cannot convert it, we will set it to null. \n",
      "                    return None \n",
      "\n",
      "        # Predicting the output\n",
      "        prediction = model.predict_proba(X[feature_list])[0]\n",
      "\n",
      "        return prediction\n",
      "    except:\n",
      "        return None"
     ]
    }
   ],
   "source": [
    "!cat ML_API/machine_learning_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading of the model occurs right before defining the endpoints:\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "# Creating the application object \n",
    "app = FastAPI()\n",
    "\n",
    "# Loading the machine learning objects to memory \n",
    "ml_model, type_dict, ml_feature_list = load_ml_model()\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "By loading the objects in the following way, the objects are saved in runtime memory and are not loaded from disk everytime a new request comes in. This makes the application much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API usage flowchart\n",
    "\n",
    "A typical flow of the API is the following: \n",
    "\n",
    "* Register a user: \n",
    "\n",
    "![registration](media/registration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the registration logic is a JWT token which we attach in each of the requests to our API. \n",
    "\n",
    "* Prediction flow: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![api-flow](media/API-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each request to the API needs to have the JWT token attached to it. Then, along with the token, the data for the API is sent ant the following flow starts: \n",
    "\n",
    "1) The user is beeing authenticated. \n",
    "\n",
    "2) If the user is authenticated, then the request data is beeing validated for the ML model. \n",
    "\n",
    "3) If the data is good, then the prediction is beeing made.\n",
    "\n",
    "4) The final response is sent. \n",
    "\n",
    "Along the way, the information is logged to the **Requests** and **Responses** tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code is available in the **app.py** script in the ML_API directory so lets try and apply the above flowchart!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests making  \n",
    "import requests \n",
    "\n",
    "# Defining the constants for the API\n",
    "url = 'http://localhost:8081'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 409; Response: {'message': 'User already exists', 'user_id': 5}\n"
     ]
    }
   ],
   "source": [
    "# Defining the user dict \n",
    "user_dict = {\n",
    "    \"username\": \"eligijus_bujokas\",\n",
    "    \"password\": \"password\",\n",
    "    \"email\": \"eligijus@testmail.com\"\n",
    "}\n",
    "\n",
    "# Sending the post request to the running API \n",
    "response = requests.post(f\"{url}/register-user\", json=user_dict)\n",
    "\n",
    "# Getting the user id \n",
    "user_id = response.json().get(\"user_id\")\n",
    "\n",
    "# Printing the response \n",
    "print(f\"Response code: {response.status_code}; Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200; JWT token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE2NDIxOTY1NjksImlhdCI6MTY0MjE5Mjk2OSwic3ViIjo1fQ.T_Io9Gx-SUDlDrLzNVKsEuthot5kgkTPWQZQO8e-bfo\n"
     ]
    }
   ],
   "source": [
    "# Querying the API for the token \n",
    "response = requests.post(f\"{url}/token\", json=user_dict)\n",
    "\n",
    "# Extracting the token from the response\n",
    "token = response.json().get(\"token\")\n",
    "\n",
    "# Printing the response\n",
    "print(f\"Response code: {response.status_code}; JWT token: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the predictions\n",
    "\n",
    "We need to first recap what was the input used to train the model. The features were: \n",
    "\n",
    "```\n",
    "{\n",
    "  \"input_schema\": {\n",
    "    \"columns\": [\n",
    "      {\n",
    "        \"name\": \"age\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"creatinine_phosphokinase\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"ejection_fraction\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"platelets\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"serum_creatinine\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"serum_sodium\",\n",
    "        \"type\": \"numeric\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"sex\",\n",
    "        \"type\": \"boolean\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"high_blood_pressure\",\n",
    "        \"type\": \"boolean\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a POST request to get the probabilities because we want to send the features and their values not as a collection of URL parameters but as a JSON object in the request body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200; Response: {'yhat_prob': '0.5124506', 'yhat': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Creating the input dictionary\n",
    "X = {\n",
    "    'age': 25,\n",
    "    'creatinine_phosphokinase': 1000,\n",
    "    'ejection_fraction': 35,\n",
    "    'platelets': 500000,\n",
    "    'serum_creatinine': 8,\n",
    "    'serum_sodium': 135,\n",
    "    'sex': 1,\n",
    "    'high_blood_pressure': 0\n",
    "}\n",
    "\n",
    "# Creating the header with the token \n",
    "header = {\n",
    "    'Authorization': token\n",
    "}\n",
    "\n",
    "# Sending the request \n",
    "response = requests.post(f\"{url}/predict\", json=X, headers=header)\n",
    "\n",
    "# Infering the response\n",
    "print(f\"Response code: {response.status_code}; Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response dictionary has two keys: \n",
    "\n",
    "`yhat_prob` - probability of a death event \n",
    "\n",
    "`yhat` - the predicted class; 1 - death_event, 0 - no_death_event\n",
    "\n",
    "The function `predict_ml` from the **app.py** file handles the request and the whole logic is presented here. \n",
    "\n",
    "The steps are: \n",
    "\n",
    "1) Extract the token \n",
    "\n",
    "2) Authenticate it\n",
    "\n",
    "3) Extract the inputs\n",
    "\n",
    "4) Log the request to database \n",
    "\n",
    "5) Make the prediction \n",
    "\n",
    "6) Log the response to database\n",
    "\n",
    "7) Return the response to the user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the results\n",
    "\n",
    "All the sufficient information for tracking the API is in the **Users**, **Requests** and **Responses** tables. We can inspect them after our run of requests and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in the database:\n",
      "   id          username                                           password  \\\n",
      "0   3              test  gAAAAABh0ZB3XUVqtNj0SIkgKzFmRey8Tt2et57V9GoAs2...   \n",
      "1   4          eligijus  gAAAAABh0ZTKq-0Es9-BzQd-KJ2R8dNY70vFlj3OY_hsC2...   \n",
      "2   5  eligijus_bujokas  gAAAAABh3uroEwCl80d480inMXMCE1TuHjaBYMgmJI3eKL...   \n",
      "\n",
      "                   email           created_datetime  \\\n",
      "0      test@testmail.com 2022-01-02 13:45:59.012700   \n",
      "1  eligijus@testmail.com 2022-01-02 14:04:26.751085   \n",
      "2  eligijus@testmail.com 2022-01-12 16:51:20.534979   \n",
      "\n",
      "            updated_datetime  enabled  \n",
      "0 2022-01-02 13:45:59.072225    False  \n",
      "1 2022-01-02 14:04:26.751085     True  \n",
      "2 2022-01-12 16:51:20.534979     True  \n",
      "--\n",
      "Last 5 requests:\n",
      "    id  user_id                                              input  \\\n",
      "17  18        5  {\"age\": 25, \"creatinine_phosphokinase\": 1000, ...   \n",
      "18  19        5  {\"age\": 25, \"creatinine_phosphokinase\": 1000, ...   \n",
      "19  20        5  {\"age\": 25, \"creatinine_phosphokinase\": 1000, ...   \n",
      "20  21        5  {\"age\": 25, \"creatinine_phosphokinase\": 1000, ...   \n",
      "21  22        5  {\"age\": 25, \"creatinine_phosphokinase\": 1000, ...   \n",
      "\n",
      "             created_datetime           updated_datetime  \n",
      "17 2022-01-12 20:45:31.866924 2022-01-12 20:45:31.866924  \n",
      "18 2022-01-12 20:54:14.030613 2022-01-12 20:54:14.030613  \n",
      "19 2022-01-14 20:36:34.018803 2022-01-14 20:36:34.018803  \n",
      "20 2022-01-14 20:37:26.669542 2022-01-14 20:37:26.669542  \n",
      "21 2022-01-14 20:42:49.979033 2022-01-14 20:42:49.979033  \n",
      "--\n",
      "Last 5 responses:\n",
      "    id  request_id                                   output  \\\n",
      "13  14          18  {\"yhat_prob\": \"0.5124506\", \"yhat\": \"1\"}   \n",
      "14  15          19  {\"yhat_prob\": \"0.5124506\", \"yhat\": \"1\"}   \n",
      "15  16          20  {\"yhat_prob\": \"0.5124506\", \"yhat\": \"1\"}   \n",
      "16  17          21  {\"yhat_prob\": \"0.5124506\", \"yhat\": \"1\"}   \n",
      "17  18          22  {\"yhat_prob\": \"0.5124506\", \"yhat\": \"1\"}   \n",
      "\n",
      "             created_datetime           updated_datetime  \n",
      "13 2022-01-12 20:45:31.904598 2022-01-12 20:45:31.904598  \n",
      "14 2022-01-12 20:54:14.048568 2022-01-12 20:54:14.048568  \n",
      "15 2022-01-14 20:36:34.059420 2022-01-14 20:36:34.059420  \n",
      "16 2022-01-14 20:37:26.694771 2022-01-14 20:37:26.694771  \n",
      "17 2022-01-14 20:42:50.004279 2022-01-14 20:42:50.004279  \n"
     ]
    }
   ],
   "source": [
    "# Importing the connection \n",
    "import pandas as pd \n",
    "from ML_API.database import engine\n",
    "\n",
    "# There maybe some legacy users beside eligjus_bujokas\n",
    "users = pd.read_sql('select * from users', engine)\n",
    "print(f\"Users in the database:\\n{users}\")\n",
    "\n",
    "# Tail of the requests\n",
    "requests_data = pd.read_sql('select * from requests', engine)\n",
    "print(f\"--\\nLast 5 requests:\\n{requests_data.tail(5)}\")\n",
    "\n",
    "# Tail of the responses\n",
    "response_data = pd.read_sql('select * from responses', engine)\n",
    "print(f\"--\\nLast 5 responses:\\n{response_data.tail(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "702d029752c7c667e866081f4be82ec9765259a2e8484bced05e549319c2e426"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('api_book': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
